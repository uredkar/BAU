{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "069cfc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b12817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2561be27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "import pyspark\n",
    "\n",
    "#from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "#spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Data\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "\n",
    "# Columns\n",
    "columns = [\"language\",\"users_count\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data).toDF(*columns)\n",
    "\n",
    "# Print DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81474c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from pyspark.sql import SparkSession\n",
    "#spark=SparkSession.builder.appName(\"accumulator\").getOrCreate()\n",
    "\n",
    "accum=spark.sparkContext.accumulator(0)\n",
    "rdd=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value) #Accessed by driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8855104b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From local[5] : 8\n",
      "parallelize : 6\n",
      "TextFile : 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create spark session with local[5]\n",
    "rdd = spark.sparkContext.parallelize(range(0,20))\n",
    "print(\"From local[5] : \"+str(rdd.getNumPartitions()))\n",
    "\n",
    "# Use parallelize with 6 partitions\n",
    "rdd1 = spark.sparkContext.parallelize(range(0,25), 6)\n",
    "print(\"parallelize : \"+str(rdd1.getNumPartitions()))\n",
    "\n",
    "rddFromFile = spark.sparkContext.textFile(r\"C:\\sources\\azure\\pyspark-examples\\resources\\test.txt\",10)\n",
    "print(\"TextFile : \"+str(rddFromFile.getNumPartitions()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c85fba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import rmtree\n",
    "rmtree(\"c:/data/partition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3cbce3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1.saveAsTextFile(\"c:/data/partition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8077040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "#rdd = spark.parallelize([1,2,3,4,5,6,7,8,9,10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57600da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49e80992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg’s\n",
      "Alice’s Adventures in Wonderland\n",
      "Project Gutenberg’s\n",
      "Adventures in Wonderland\n",
      "Project Gutenberg’s\n"
     ]
    }
   ],
   "source": [
    "data = [\"Project Gutenberg’s\",\n",
    "        \"Alice’s Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\",\n",
    "        \"Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\"]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "for element in rdd.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bc60e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg’s\n",
      "Alice’s Adventures in Wonderland\n",
      "Project Gutenberg’s\n",
      "Adventures in Wonderland\n",
      "Project Gutenberg’s\n"
     ]
    }
   ],
   "source": [
    "for element in rdd.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1857291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project\n",
      "Gutenberg’s\n",
      "Alice’s\n",
      "Adventures\n",
      "in\n",
      "Wonderland\n",
      "Project\n",
      "Gutenberg’s\n",
      "Adventures\n",
      "in\n",
      "Wonderland\n",
      "Project\n",
      "Gutenberg’s\n"
     ]
    }
   ],
   "source": [
    "rdd2=rdd.flatMap(lambda x: x.split(\" \"))\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "786379a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- col: string (nullable = true)\n",
      "\n",
      "+---------+------+\n",
      "|     name|   col|\n",
      "+---------+------+\n",
      "|    James|  Java|\n",
      "|    James| Scala|\n",
      "|  Michael| Spark|\n",
      "|  Michael|  Java|\n",
      "|  Michael|  null|\n",
      "|   Robert|CSharp|\n",
      "|   Robert|      |\n",
      "|Jefferson|     1|\n",
      "|Jefferson|     2|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})]\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(df.name,explode(df.knownLanguages))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc4c6af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+\n",
      "|      name|     knownLanguages|          properties|\n",
      "+----------+-------------------+--------------------+\n",
      "|     James|      [Java, Scala]|{eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java, null]|{eye -> null, hai...|\n",
      "|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n",
      "|Washington|               null|                null|\n",
      "| Jefferson|             [1, 2]|                  {}|\n",
      "+----------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f3923ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|   Name|Age|Occupation|\n",
      "+-------+---+----------+\n",
      "|  Alice| 25|  Engineer|\n",
      "|Charlie| 28|  Designer|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"InnerJoinExample\").getOrCreate()\n",
    "\n",
    "# Create DataFrame 1\n",
    "data1 = [(\"Alice\", 25),\n",
    "         (\"Bob\", 30),\n",
    "         (\"Charlie\", 28)]\n",
    "columns1 = [\"Name\", \"Age\"]\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "\n",
    "# Create DataFrame 2\n",
    "data2 = [(\"Alice\", \"Engineer\"),\n",
    "         (\"Charlie\", \"Designer\"),\n",
    "         (\"David\", \"Analyst\")]\n",
    "columns2 = [\"Name\", \"Occupation\"]\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "# Perform inner join\n",
    "result_df = df1.join(df2, on=\"Name\", how=\"inner\")\n",
    "\n",
    "# Show the result\n",
    "result_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09622d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+--------+----------+----------+-----------+\n",
      "|   Name| Age|Occupation|Category|AgePlusTwo|AgeSquared|AgeCategory|\n",
      "+-------+----+----------+--------+----------+----------+-----------+\n",
      "|  Alice|  25|      null|   Young|        27|       625|      Young|\n",
      "|    Bob|null|  Engineer|   Adult|      null|      null|      Adult|\n",
      "|Charlie|  28|  Designer|   Young|        30|       784|      Young|\n",
      "+-------+----+----------+--------+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import col \n",
    "data = [(\"Alice\", 25, None),\n",
    "        (\"Bob\", None, \"Engineer\"),\n",
    "        (\"Charlie\", 28, \"Designer\")]\n",
    "columns = [\"Name\", \"Age\", \"Occupation\"]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Add a new column using when() function\n",
    "df2 = df.withColumn(\"Category\", when(df.Age < 30, \"Young\").otherwise(\"Adult\"))\n",
    "result_df = df2.withColumn(\"AgePlusTwo\", expr(\"Age + 2\")) \\\n",
    "              .withColumn(\"AgeSquared\", expr(\"Age * Age\")) \\\n",
    "              .withColumn(\"AgeCategory\", expr(\"CASE WHEN Age < 30 THEN 'Young' ELSE 'Adult' END\"))\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f3a4185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+-------------+\n",
      "|   Name| Age|Occupation|NewOccupation|\n",
      "+-------+----+----------+-------------+\n",
      "|  Alice|  25|      null|      Unknown|\n",
      "|    Bob|null|  Engineer|     Engineer|\n",
      "|Charlie|  28|  Designer|     Designer|\n",
      "+-------+----+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = df.withColumn(\"NewOccupation\", when(col(\"Occupation\").isNull(), \"Unknown\").otherwise(col(\"Occupation\")))\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b33cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
