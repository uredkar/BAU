{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Source, use for learning\n",
    "https://gist.githubusercontent.com/shreyasms17/6c55a21aa77a8e003cefe578edd85da1/raw/35d520fe3e623f4c6a40a45b8dafdaa557507790/autoflatten.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class AutoFlatten:\n",
    "    def __init__(self, json_schema):\n",
    "        self.fields_in_json = self.get_fields_in_json(json_schema)\n",
    "        self.all_fields = {}\n",
    "        self.cols_to_explode = set()\n",
    "        self.structure = {}\n",
    "        self.order = []\n",
    "        self.bottom_to_top = {}\n",
    "        self.rest = set()\n",
    "        \n",
    "        \n",
    "    def get_fields_in_json(self, json_schema):\n",
    "        '''\n",
    "        Description: \n",
    "        This function takes in the schema in json format and returns the metadata of the schema\n",
    "\n",
    "        :param json_schema: [type : str] a string containing path to raw data\n",
    "        :return fields: [type : dict] contains metadata of the schema\n",
    "        '''\n",
    "        a = json_schema.json()\n",
    "        schema_dict = json.loads(a)\n",
    "        fields = schema_dict['fields']\n",
    "        return fields\n",
    "        \n",
    "        \n",
    "    def is_leaf(self, data):\n",
    "        '''\n",
    "        Description: \n",
    "        This function checks if the particular field in the schema is a leaf or not.\n",
    "        Types not considered as a leaf : struct, array\n",
    "\n",
    "        :param data: [type: dict] a dictionary containing metadata about a field\n",
    "\n",
    "        :return leaf: [type: bool] indicates whether a given field is a leaf or not\n",
    "        '''\n",
    "        try:\n",
    "            if isinstance(data['type'], str):\n",
    "                leaf = True if data['type'] != 'struct' else False\n",
    "            else:\n",
    "                leaf = True if data['type']['type'] == 'map' else False\n",
    "        except:\n",
    "            leaf = False\n",
    "        finally:\n",
    "            return leaf\n",
    "            \n",
    "            \n",
    "    def unnest_dict(self, json, cur_path):\n",
    "        '''\n",
    "        Description: \n",
    "        This function unnests the dictionaries in the json schema recursively \n",
    "        and maps the hierarchical path to the field to the column name when it encounters a leaf node\n",
    "\n",
    "        :param json: [type: dict/list] contains metadata about a field\n",
    "        :param cur_path: [type: str] contains hierarchical path to that field, each parent separated by a '.'\n",
    "        '''\n",
    "        if self.is_leaf(json):\n",
    "            self.all_fields[f\"{cur_path}.{json['name']}\"] = json['name']\n",
    "            return\n",
    "        else:\n",
    "            if isinstance(json, list):\n",
    "                for i in range(len(json)):\n",
    "                    self.unnest_dict(json[i], cur_path)\n",
    "            elif isinstance(json, dict):\n",
    "                if isinstance(json['type'], str):\n",
    "                    cur_path = f\"{cur_path}.{json['name']}\" if json['type'] != 'struct' else cur_path\n",
    "                    self.unnest_dict(json['type'], cur_path)\n",
    "                else:\n",
    "                    if json['type']['type'] == 'array':\n",
    "                        cur_path = f\"{cur_path}.{json['name']}\"\n",
    "                        if isinstance(json['type']['elementType'], dict):\n",
    "                            self.cols_to_explode.add(cur_path)\n",
    "                            self.unnest_dict(json['type']['elementType']['fields'], cur_path)\n",
    "                        else:\n",
    "                            self.cols_to_explode.add(cur_path)\n",
    "                            self.all_fields[f\"{cur_path}\"] = json['name']\n",
    "                            return\n",
    "                    elif json['type']['type'] == 'struct':\n",
    "                        cur_path = f\"{cur_path}.{json['name']}\"\n",
    "                        self.unnest_dict(json['type']['fields'], cur_path)\n",
    "                        \n",
    "                        \n",
    "    def get_structure(self, col_list):\n",
    "        '''\n",
    "        Description: \n",
    "        This function gets the structure to the traversal to array field in the schema\n",
    "\n",
    "        :param col_list: [type: list] contains list of fields that are to be exploded\n",
    "\n",
    "        :return structure: [type: dict] contains the hierarchical mapping for array fields\n",
    "        '''\n",
    "        structure = {'json' : {}}\n",
    "        for val in col_list:\n",
    "            arr = val.split('.')\n",
    "            a = structure['json']\n",
    "            for i in range(1,len(arr)):\n",
    "                if not a.__contains__(arr[i]):\n",
    "                    a[arr[i]] = {} \n",
    "                a = a[arr[i]]\n",
    "        return structure\n",
    "        \n",
    "        \n",
    "    \n",
    "    def extract_order(self, structure):\n",
    "        '''\n",
    "        Description: \n",
    "        This function does a BFS traversal to obtain the order in which \n",
    "        the array type fields are to be exploded\n",
    "\n",
    "        :param structure: [type: dict] contains the hierarchical mapping for array fields\n",
    "\n",
    "        :return order: [type: list] contains the fields in order in which array explode has to take place\n",
    "        '''\n",
    "        q = [('', structure['json'])]\n",
    "        order = []\n",
    "        while q:\n",
    "            key, a = q.pop(0)\n",
    "            for x in a.keys():\n",
    "                order.append(f\"{key}.{x}\")\n",
    "                q.append((f\"{key}.{x}\", a[x]))\n",
    "        return order\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_bottom_to_top(self, order, all_cols_in_explode_cols):\n",
    "        '''\n",
    "        Description: \n",
    "        This function gets the mutually exclusive leaf fields in every array type column\n",
    "\n",
    "        :param order: [type: list] contains the fields in order in which array explode has to take place\n",
    "        :param all_cols_in_explode_cols: [type: set] contains all fields in array type fields\n",
    "\n",
    "        :return bottom_to_top: [type: dict] contains list of mutually exclusive leaf fields for every \n",
    "                                array type / struct type (parent to array type) field\n",
    "        '''\n",
    "        bottom_to_top = {}\n",
    "        for column in reversed(order):\n",
    "            x_cols = set(filter(lambda x: x.startswith(column), list(all_cols_in_explode_cols)))\n",
    "            bottom_to_top[column] = list(x_cols)\n",
    "            all_cols_in_explode_cols = all_cols_in_explode_cols.difference(x_cols)\n",
    "        return bottom_to_top\n",
    "        \n",
    "        \n",
    "        \n",
    "    def compute(self):\n",
    "        '''\n",
    "        Description: \n",
    "        This function performs the required computation and gets all the resources \n",
    "        needed for further process of selecting and exploding fields\n",
    "        '''\n",
    "        self.unnest_dict(self.fields_in_json, '')\n",
    "        all_cols_in_explode_cols = set(filter(lambda x: x.startswith(tuple(self.cols_to_explode)), self.all_fields.keys()))\n",
    "        self.rest = set(self.all_fields.keys()).difference(all_cols_in_explode_cols)\n",
    "        self.structure = self.get_structure([f\"json{x}\" for x in list(self.cols_to_explode)])\n",
    "        self.order = self.extract_order(self.structure)\n",
    "        self.bottom_to_top = self.get_bottom_to_top(self.order, all_cols_in_explode_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode_outer\n",
    "from pyspark.sql.types import *\n",
    "from copy import deepcopy\n",
    "from autoflatten import AutoFlatten\n",
    "from collections import Counter\n",
    "\n",
    "s3_path = 's3://mybucket/orders/'\n",
    "df = spark.read.orc(s3_path)\n",
    "json_df = spark.read.json(df.rdd.map(lambda row: row.json))\n",
    "json_schema = json_df.schema\n",
    "\n",
    "af = AutoFlatten(json_schema)\n",
    "af.compute()\n",
    "\n",
    "df1 = json_df\n",
    "\n",
    "visited = set([f'.{column}' for column in df1.columns])\n",
    "duplicate_target_counter = Counter(af.all_fields.values())\n",
    "cols_to_select = df1.columns\n",
    "for rest_col in af.rest:\n",
    "    if rest_col not in visited:\n",
    "        cols_to_select += [rest_col[1:]] if (duplicate_target_counter[af.all_fields[rest_col]]==1 and af.all_fields[rest_col] not in df1.columns) else [col(rest_col[1:]).alias(f\"{rest_col[1:].replace('.', '>')}\")]\n",
    "        visited.add(rest_col)\n",
    "\n",
    "df1 = df1.select(cols_to_select)\n",
    "\n",
    "\n",
    "if af.order:\n",
    "    for key in af.order:\n",
    "        column = key.split('.')[-1]\n",
    "        if af.bottom_to_top[key]:\n",
    "            #########\n",
    "            #values for the column in bottom_to_top dict exists if it is an array type\n",
    "            #########\n",
    "            df1 = df1.select('*', explode_outer(col(column)).alias(f\"{column}_exploded\")).drop(column)\n",
    "            data_type = df1.select(f\"{column}_exploded\").schema.fields[0].dataType\n",
    "            if not (isinstance(data_type, StructType) or isinstance(data_type, ArrayType)):\n",
    "                df1 = df1.withColumnRenamed(f\"{column}_exploded\", column if duplicate_target_counter[af.all_fields[key]]<=1 else key[1:].replace('.', '>'))\n",
    "                visited.add(key)\n",
    "            else:\n",
    "                #grabbing all paths to columns after explode\n",
    "                cols_in_array_col = set(map(lambda x: f'{key}.{x}', df1.select(f'{column}_exploded.*').columns))\n",
    "                #retrieving unvisited columns\n",
    "                cols_to_select_set = cols_in_array_col.difference(visited)\n",
    "                all_cols_to_select_set = set(af.bottom_to_top[key])\n",
    "                #check done for duplicate column name & path\n",
    "                cols_to_select_list = list(map(lambda x: f\"{column}_exploded{'.'.join(x.split(key)[1:])}\" if (duplicate_target_counter[af.all_fields[x]]<=1 and x.split('.')[-1] not in df1.columns) else col(f\"{column}_exploded{'.'.join(x.split(key)[1:])}\").alias(f\"{x[1:].replace('.', '>')}\"), list(all_cols_to_select_set)))\n",
    "                #updating visited set\n",
    "                visited.update(cols_to_select_set)\n",
    "                rem = list(map(lambda x: f\"{column}_exploded{'.'.join(x.split(key)[1:])}\", list(cols_to_select_set.difference(all_cols_to_select_set))))\n",
    "                df1 = df1.select(df1.columns + cols_to_select_list + rem).drop(f\"{column}_exploded\")        \n",
    "        else:\n",
    "            #########\n",
    "            #values for the column in bottom_to_top dict do not exist if it is a struct type / array type containing a string type\n",
    "            #########\n",
    "            #grabbing all paths to columns after opening\n",
    "            cols_in_array_col = set(map(lambda x: f'{key}.{x}', df1.selectExpr(f'{column}.*').columns))\n",
    "            #retrieving unvisited columns\n",
    "            cols_to_select_set = cols_in_array_col.difference(visited)\n",
    "            #check done for duplicate column name & path\n",
    "            cols_to_select_list = list(map(lambda x: f\"{column}.{x.split('.')[-1]}\" if (duplicate_target_counter[x.split('.')[-1]]<=1 and x.split('.')[-1] not in df1.columns) else col(f\"{column}.{x.split('.')[-1]}\").alias(f\"{x[1:].replace('.', '>')}\"), list(cols_to_select_set)))\n",
    "            #updating visited set\n",
    "            visited.update(cols_to_select_set)\n",
    "            df1 = df1.select(df1.columns + cols_to_select_list).drop(f\"{column}\")\n",
    "\n",
    " \n",
    "final_df = df1.select([field[1:].replace('.', '>') if duplicate_target_counter[af.all_fields[field]]>1 else af.all_fields[field] for field in af.all_fields])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
